{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "print(np.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a2c034eff2207f0e",
   "metadata": {},
   "source": [
    "class PostcardDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Filter\n",
    "        self.df = self.df.dropna(subset=[\"akon_id\", \"country_id\", \"latitude\", \"longitude\"])\n",
    "\n",
    "        self.countries = sorted(self.df[\"country_id\"].unique())\n",
    "        self.country_to_idx = {c: i for i, c in enumerate(self.countries)}\n",
    "        self.idx_to_country = {i: c for c, i in self.country_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, row[\"akon_id\"] + \".jpg\")\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        country_idx = self.country_to_idx[row[\"country_id\"]]\n",
    "        lat = float(row[\"latitude\"])\n",
    "        lon = float(row[\"longitude\"])\n",
    "\n",
    "        # Normierte Koordinaten\n",
    "        coords = torch.tensor([lat / 90.0, lon / 180.0], dtype=torch.float32)\n",
    "\n",
    "        return image, country_idx, coords"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f120cb8c714d083c",
   "metadata": {},
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "csv_path = \"akon_postcards_public_domain.csv\"\n",
    "image_dir = \"images/256\"\n",
    "dataset = PostcardDataset(csv_path, image_dir, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Beispielbild anzeigen\n",
    "img, label_idx, coords = dataset[0]\n",
    "plt.imshow(img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "plt.title(f\"Label-Index: {label_idx}\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44d3face32133e9d",
   "metadata": {},
   "source": [
    "embedding_dim = 48\n",
    "z_dim = 100\n",
    "coord_dim = 2\n",
    "num_countries = len(dataset.country_to_idx)\n",
    "\n",
    "country_embedding = nn.Embedding(num_countries, embedding_dim).to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b81647f48983f61",
   "metadata": {},
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim + embedding_dim + coord_dim, 1024 * 4 * 4),\n",
    "            nn.BatchNorm1d(1024 * 4 * 4),  # BatchNorm hinzugefügt\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, country_embed, coords):\n",
    "        x = torch.cat([z, country_embed, coords], dim=1)\n",
    "        x = self.fc(x).view(-1, 1024, 4, 4)\n",
    "        return self.conv(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae419a0b10ab4c9",
   "metadata": {},
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # KLEINERE Label-Projektion - nur auf 32x32!\n",
    "        self.label_proj = nn.Sequential(\n",
    "            nn.Linear(embedding_dim + coord_dim, 32 * 32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: 3 (image) + 1 (label map) = 4 channels\n",
    "            nn.Conv2d(4, 32, 4, 2, 1),  # 256->128\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.25),  # Dropout hinzugefügt\n",
    "\n",
    "            nn.Conv2d(32, 64, 4, 2, 1),  # 128->64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.25),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),  # 64->32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout2d(0.25),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),  # 32->16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),  # 16->8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 2, 1),  # 8->4\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global Average Pooling\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, country_embed, coords):\n",
    "        # Label-Map erst klein erstellen (32x32) - VIEL effizienter!\n",
    "        label = torch.cat([country_embed, coords], dim=1)\n",
    "        label_map = self.label_proj(label).view(-1, 1, 32, 32)\n",
    "        \n",
    "        # Dann auf Bildgröße hochskalieren (256x256) - GLEICHE QUALITÄT!\n",
    "        label_map = F.interpolate(label_map, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Input: 256x256 Bild + 256x256 Label-Map = 4 Kanäle\n",
    "        x = torch.cat([img, label_map], dim=1)  # Shape: [batch, 4, 256, 256]\n",
    "        return self.model(x)  # Output: [batch, 1] (Real/Fake Prediction)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21a25a80e795e16a",
   "metadata": {},
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# VERSCHIEDENE LEARNING RATES - Generator höher, Diskriminator niedriger\n",
    "g_opt = torch.optim.Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "d_opt = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=(0.5, 0.999))  # Halbe LR!\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "g_scheduler = torch.optim.lr_scheduler.ExponentialLR(g_opt, gamma=0.995)\n",
    "d_scheduler = torch.optim.lr_scheduler.ExponentialLR(d_opt, gamma=0.995)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c1a05a8854d8e4f",
   "metadata": {},
   "source": [
    "def train_step(imgs, country_idxs, coords):\n",
    "    batch_size = imgs.size(0)\n",
    "    \n",
    "    real_labels = torch.ones(batch_size, 1).to(device) * 0.9\n",
    "    fake_labels = torch.zeros(batch_size, 1).to(device) + 0.1\n",
    "    \n",
    "    country_embed = country_embedding(country_idxs).detach()\n",
    "    coords = coords.detach()    \n",
    "    \n",
    "    d_loss = 0\n",
    "    real_pred_mean = 0\n",
    "    fake_pred_mean = 0\n",
    "    \n",
    "    if np.random.random() > 0.2:\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake_imgs = generator(z, country_embed, coords)\n",
    "        \n",
    "        real_preds = discriminator(imgs, country_embed, coords)\n",
    "        fake_preds = discriminator(fake_imgs.detach(), country_embed, coords)\n",
    "        \n",
    "        d_loss = criterion(real_preds, real_labels) + criterion(fake_preds, fake_labels)\n",
    "        \n",
    "        d_opt.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_opt.step()\n",
    "        \n",
    "        real_pred_mean = real_preds.mean().item()\n",
    "        fake_pred_mean = fake_preds.mean().item()\n",
    "        d_loss = d_loss.item()\n",
    "    \n",
    "    g_losses = []\n",
    "    for i in range(2):\n",
    "        z = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake_imgs = generator(z, country_embed, coords)\n",
    "        fake_preds = discriminator(fake_imgs, country_embed, coords)\n",
    "        g_loss = criterion(fake_preds, real_labels)\n",
    "\n",
    "        g_opt.zero_grad()\n",
    "        if i == 0:\n",
    "            g_loss.backward(retain_graph=True)  # <-- hier\n",
    "        else:\n",
    "            g_loss.backward()\n",
    "        g_opt.step()\n",
    "        g_losses.append(g_loss.item())\n",
    "\n",
    "        \n",
    "    return d_loss, np.mean(g_losses), real_pred_mean, fake_pred_mean\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "950abb0d",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "log_file = \"training/DCGANv3/training_metrics.csv\"\n",
    "os.makedirs(\"training/DCGANv3\", exist_ok=True)\n",
    "\n",
    "# Falls Datei noch nicht existiert, Header schreiben\n",
    "if not os.path.exists(log_file):\n",
    "    with open(log_file, mode='w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Epoch\", \"D_Loss\", \"G_Loss\", \"Real_Pred\", \"Fake_Pred\", \"G_LR\", \"D_LR\", \"Duration\"])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3430f48dc15a4eb",
   "metadata": {},
   "source": [
    "import time\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Ordner vorbereiten\n",
    "os.makedirs(\"training/DCGANv3/epoche_bilder\", exist_ok=True)\n",
    "os.makedirs(\"training/DCGANv3/epoche_schritte\", exist_ok=True)\n",
    "\n",
    "# Feste Inputs für gleichbleibende Samples\n",
    "fixed_noise = torch.randn(16, z_dim).to(device)\n",
    "fixed_country = torch.randint(0, num_countries, (16,)).to(device)\n",
    "fixed_coords = torch.rand(16, coord_dim).to(device) * 2 - 1  # [-1, 1] range\n",
    "\n",
    "# Checkpoint Setup\n",
    "start_epoch = 0\n",
    "checkpoint_path = \"training/DCGANv3/epoche_schritte/checkpoint_epoch_latest.pth\"\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\" Checkpoint gefunden, lade...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "    discriminator.load_state_dict(checkpoint[\"discriminator_state_dict\"])\n",
    "    g_opt.load_state_dict(checkpoint[\"g_opt_state_dict\"])\n",
    "    d_opt.load_state_dict(checkpoint[\"d_opt_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"]\n",
    "    print(f\"⏮ Weiter bei Epoche {start_epoch}\")\n",
    "else:\n",
    "    print(\" Kein Checkpoint gefunden, starte bei Epoche 0.\")\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n Starte Epoche {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    epoch_d_losses = []\n",
    "    epoch_g_losses = []\n",
    "    epoch_real_preds = []\n",
    "    epoch_fake_preds = []\n",
    "\n",
    "    for batch_idx, (imgs, country_idxs, coords) in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        country_idxs = country_idxs.to(device)\n",
    "        coords = coords.to(device)\n",
    "\n",
    "        # Verbessertes Training\n",
    "        d_loss, g_loss, real_pred, fake_pred = train_step(imgs, country_idxs, coords)\n",
    "        \n",
    "        epoch_d_losses.append(d_loss)\n",
    "        epoch_g_losses.append(g_loss)\n",
    "        if real_pred > 0:  # Nur wenn Diskriminator trainiert wurde\n",
    "            epoch_real_preds.append(real_pred)\n",
    "            epoch_fake_preds.append(fake_pred)\n",
    "\n",
    "    # Learning Rate Update\n",
    "    g_scheduler.step()\n",
    "    d_scheduler.step()\n",
    "\n",
    "    #  Epoch Zusammenfassung\n",
    "    duration = time.time() - start_time\n",
    "    avg_d_loss = np.mean(epoch_d_losses)\n",
    "    avg_g_loss = np.mean(epoch_g_losses)\n",
    "    avg_real_pred = np.mean(epoch_real_preds) if epoch_real_preds else 0\n",
    "    avg_fake_pred = np.mean(epoch_fake_preds) if epoch_fake_preds else 0\n",
    "    \n",
    "    print(f\" Epoche {epoch+1} | D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}\")\n",
    "    print(f\" Real Pred: {avg_real_pred:.3f} | Fake Pred: {avg_fake_pred:.3f} | {duration:.2f}s\")\n",
    "    print(f\" G LR: {g_scheduler.get_last_lr()[0]:.6f} | D LR: {d_scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    #  In CSV loggen\n",
    "    with open(log_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            epoch + 1,\n",
    "            avg_d_loss,\n",
    "            avg_g_loss,\n",
    "            avg_real_pred,\n",
    "            avg_fake_pred,\n",
    "            g_scheduler.get_last_lr()[0],\n",
    "            d_scheduler.get_last_lr()[0],\n",
    "            duration\n",
    "        ])\n",
    "\n",
    "\n",
    "    #  Alle 10 Epochen Beispielbilder speichern\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            embed = country_embedding(fixed_country)\n",
    "            samples = generator(fixed_noise, embed, fixed_coords)\n",
    "            save_image(samples * 0.5 + 0.5,\n",
    "                       f\"training/DCGANv3/epoche_bilder/gen_samples_epoch_{epoch+1}.png\",\n",
    "                       nrow=4)\n",
    "        generator.train()\n",
    "\n",
    "    # Alle 50 Epochen Checkpoint speichern\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_data = {\n",
    "            'epoch': epoch + 1,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'g_opt_state_dict': g_opt.state_dict(),\n",
    "            'd_opt_state_dict': d_opt.state_dict(),\n",
    "            'g_scheduler_state_dict': g_scheduler.state_dict(),\n",
    "            'd_scheduler_state_dict': d_scheduler.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint_data, f\"training/DCGANv3/epoche_schritte/checkpoint_epoch_{epoch+1}.pth\")\n",
    "        torch.save(checkpoint_data, checkpoint_path)  # aktuellsten Stand überschreiben\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "50fdea03fa1b2044",
   "metadata": {},
   "source": [
    "# Am Ende des Trainings (nach der Schleife)\n",
    "torch.save(generator.state_dict(), \"generator_improved.pth\")\n",
    "torch.save(discriminator.state_dict(), \"discriminator_improved.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c13f9cd6d878cdf0",
   "metadata": {},
   "source": [
    "# Beispiel: Generiere ein Bild für ein bestimmtes Land und Koordinaten (z.B. Köln)\n",
    "z = torch.randn(1, z_dim).to(device)\n",
    "\n",
    "# Beispiel: normierte Koordinaten von Köln (Latitude / 90, Longitude / 180)\n",
    "# Köln ca. 50.94°N, 6.96°E\n",
    "lat_norm = 50.94 / 90.0\n",
    "lon_norm = 6.96 / 180.0\n",
    "coords = torch.tensor([[lat_norm, lon_norm]], dtype=torch.float32).to(device)\n",
    "\n",
    "# Country-ID (z.B. Deutschland \"DE\")\n",
    "city_country = \"DE\"\n",
    "country_idx = torch.tensor([dataset.country_to_idx[city_country]]).to(device)\n",
    "\n",
    "# Hole das Country-Embedding\n",
    "country_embed = country_embedding(country_idx)\n",
    "\n",
    "# Modell in Eval-Modus\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    gen_img = generator(z, country_embed, coords).squeeze().cpu()\n",
    "    plt.imshow(gen_img.permute(1, 2, 0) * 0.5 + 0.5)\n",
    "    plt.title(f\"Generierte Postkarte: {city_country}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c16b19b53a803aea",
   "metadata": {},
   "source": [
    "# Verbesserte Visualisierung der Embeddings\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "\n",
    "# Alle Länder aus dem Dataset holen\n",
    "all_countries = sorted(dataset.df[\"country_id\"].unique())\n",
    "all_idxs = torch.tensor([dataset.country_to_idx[c] for c in all_countries]).to(device)\n",
    "\n",
    "# Embeddings holen (die trainierten!)\n",
    "embeddings = country_embedding(all_idxs).detach().cpu().numpy()\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_countries)-1))\n",
    "emb_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "# DataFrame zum Plotten bauen\n",
    "df_tsne = pd.DataFrame({\n",
    "    \"country\": all_countries,\n",
    "    \"x\": emb_2d[:, 0],\n",
    "    \"y\": emb_2d[:, 1]\n",
    "})\n",
    "\n",
    "# Plot t-SNE\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(df_tsne[\"x\"], df_tsne[\"y\"], alpha=0.7)\n",
    "\n",
    "for i, country in enumerate(df_tsne[\"country\"]):\n",
    "    plt.annotate(country, (df_tsne.loc[i, \"x\"], df_tsne.loc[i, \"y\"]), fontsize=8)\n",
    "\n",
    "plt.title(\"Trainierte Country Embeddings via t-SNE\")\n",
    "plt.show()\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "df_pca = pd.DataFrame({\n",
    "    \"country\": all_countries,\n",
    "    \"x\": emb_2d_pca[:, 0],\n",
    "    \"y\": emb_2d_pca[:, 1]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(df_pca[\"x\"], df_pca[\"y\"], alpha=0.7)\n",
    "\n",
    "for i, country in enumerate(df_pca[\"country\"]):\n",
    "    plt.annotate(country, (df_pca.loc[i, \"x\"], df_pca.loc[i, \"y\"]), fontsize=8)\n",
    "\n",
    "plt.title(\"Trainierte Country Embeddings via PCA\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
